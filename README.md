# ğŸ” AnÃ¡lise de Robustez, Interpretabilidade e Ataques AdversÃ¡rios no CLIP  
RepositÃ³rio Oficial â€“ Projeto de TÃ³picos Especiais  
Autor: OtÃ¡vio Augusto Cavalcanti Neto  
GitHub: https://github.com/Otavio574/Analise-Robustez-Interpretabilidade-CLIP

---

## ğŸ“Œ VisÃ£o Geral

Este repositÃ³rio contÃ©m todo o cÃ³digo utilizado para avaliar o comportamento do modelo **CLIP (ViT-B/32)** em tarefas Fine-Grained, incluindo:

- AvaliaÃ§Ã£o inicial da acurÃ¡cia Zero-Shot
- Testes de robustez (ruÃ­do, transformaÃ§Ã£o e variaÃ§Ãµes de entrada)
- AnÃ¡lise de interpretabilidade (GradCAM em mÃºltiplas camadas)
- ImplementaÃ§Ã£o e execuÃ§Ã£o de ataque adversÃ¡rio (PGD)
- GeraÃ§Ã£o de relatÃ³rios e heatmaps

O projeto acompanha o relatÃ³rio tÃ©cnico desenvolvido na disciplina de **TÃ³picos Especiais**, cujo objetivo Ã© analisar a *robustez*, a *explicabilidade* e as *vulnerabilidades* de modelos avanÃ§ados de visÃ£o-linguagem.

---

## ğŸ§  Objetivos do Projeto

1. Aplicar modelos de aprendizagem profunda (CLIP) em uma tarefa de visÃ£o computacional Fine-Grained.
2. Avaliar a robustez do modelo sob condiÃ§Ãµes adversas.
3. Investigar o comportamento interno do modelo usando tÃ©cnicas de interpretabilidade.
4. Desenvolver e aplicar ataques adversÃ¡rios.
5. **Analisar e discutir os insights obtidos sobre o comportamento interno do modelo**, conforme apresentado no CapÃ­tulo 7 do relatÃ³rio.

---

## ğŸ“ Estrutura do RepositÃ³rio

Analise-Robustez-Interpretabilidade-CLIP/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ run_first_evaluation.py # AvaliaÃ§Ã£o Zero-Shot (Top-1 / Top-5)
â”‚ â”œâ”€â”€ run_robustness.py # RuÃ­do, transformaÃ§Ãµes e variaÃ§Ãµes
â”‚ â”œâ”€â”€ run_interpretability.py # GradCAM (early, middle, late)
â”‚ â”œâ”€â”€ run_adversarial_attack.py # PGD Targeted + visualizaÃ§Ã£o
â”‚ â”œâ”€â”€ script_master.py # Executa tudo em ordem automÃ¡tica
â”‚
â”œâ”€â”€ datasets/ # Classe Aircraft (ou link externo)
â”œâ”€â”€ reports_and_results/ # Imagens, heatmaps, grÃ¡ficos e logs
â””â”€â”€ README.md


---

## ğŸ“¦ Requisitos

- Python 3.8+
- PyTorch + CUDA (opcional, mas recomendado)
- OpenAI CLIP
- torchvision
- pytorch-grad-cam
- numpy, pillow, matplotlib

InstalaÃ§Ã£o rÃ¡pida:

```bash
pip install -r requirements.txt
```
â–¶ï¸ ExecuÃ§Ã£o RÃ¡pida (Modo AutomÃ¡tico)

O repositÃ³rio inclui um script que executa todas as etapas do pipeline automaticamente:
python src/script_master.py

A ordem executada Ã©:

1. run_first_evaluation.py

2. run_robustness.py

3. run_interpretability.py

4. run_adversarial_attack.py

Todos os resultados sÃ£o armazenados em:

reports_and_results/

â–¶ï¸ ExecuÃ§Ã£o Manual (Etapa por Etapa)

AvaliaÃ§Ã£o Zero-Shot:

python src/run_first_evaluation.py


Robustez:

python src/run_robustness.py


Interpretabilidade (GradCAM):

python src/run_interpretability.py


Ataque AdversÃ¡rio (PGD Targeted):

python src/run_adversarial_attack.py

ğŸ”¥ Insights Principais (Resumo do CapÃ­tulo 7 do RelatÃ³rio)

ApÃ³s realizar interpretabilidade, robustez e ataques adversÃ¡rios, os principais insights sobre o comportamento interno do CLIP foram:

1. Foco em features nÃ£o discriminativas

O modelo ignora caracterÃ­sticas crÃ­ticas (como o nÃºmero de motores no A340 vs A330) e se concentra em regiÃµes genÃ©ricas como fuselagem, nariz e cauda.

2. DecisÃµes baseadas em semÃ¢ntica global

A camada late domina decisivamente o processo, mostrando que o CLIP se apoia mais em conceitos amplos do que em detalhes estruturais.

3. Arquitetura ViT + treinamento contrastivo limitam Fine-Grained

As descriÃ§Ãµes textuais do CLIP nÃ£o carregam granularidade suficiente para distinguir modelos da mesma famÃ­lia.

4. Vulnerabilidade a ataques adversÃ¡rios

Mesmo com perturbaÃ§Ã£o baixa (Îµ=0.0314), o CLIP foi enganado e classificou A340 como A330 â€” reforÃ§ando sua fragilidade.

5. ImplicaÃ§Ãµes prÃ¡ticas

Zero-Shot CLIP nÃ£o Ã© adequado para FGVC sem ajustes

Fine-tuning supervisionado ou few-shot ajudam

Modelos especializados (TransFG, FFVT) seriam mais apropriados para FGVC-Aircraft

ğŸ–¼ï¸ Exemplos de Resultados

O relatÃ³rio inclui imagens como:

- Imagem original vs adversarial

- PerturbaÃ§Ã£o 10Ã—

- Heatmaps GradCAM (early, middle, late)

- ComparaÃ§Ãµes de ativaÃ§Ã£o por camada

As figuras sÃ£o automaticamente salvas em:

reports_and_results/adversarial_results/

ğŸ“œ LicenÃ§a

MIT License

ğŸ“¬ Contato

Se quiser discutir sobre CLIP, interpretabilidade, ataques adversÃ¡rios ou Fine-Grained Vision, sÃ³ chamar!

